{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9NsHuKY3TArIUlYB6sOB7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raseealmanea/2025_GP_19/blob/main/Ouwn_Model/DataPreparing/Mimic_III_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LGPUnoVAltZK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c4e2585-441e-487c-9e44-a1de803cce1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "from collections import Counter\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from prepare_data.utils import (\n",
        "    TextPreprocessor,\n",
        "    format_code_dataframe,\n",
        "    get_mimiciii_notes,\n",
        "    merge_code_dataframes,\n",
        "    merge_reports_addendum,\n",
        "    preprocess_documents,\n",
        "    reformat_icd9,\n",
        "    remove_duplicated_codes,\n",
        "    replace_nans_with_empty_lists,\n",
        ")\n",
        "# Paths\n",
        "DOWNLOAD_DIRECTORY_MIMICIII = \"/content/drive/MyDrive/MIMICIII_RAW\"\n",
        "DATA_DIRECTORY_MIMICIII_CLEAN = \"/content/drive/MyDrive/MIMICIII_CLEAN\"\n",
        "\n",
        "# Columns\n",
        "SUBJECT_ID_COLUMN = \"SUBJECT_ID\"\n",
        "ID_COLUMN = \"HADM_ID\"\n",
        "TEXT_COLUMN = \"TEXT\"\n",
        "TARGET_COLUMN = \"LABELS\"\n",
        "\n",
        "CODE_SYSTEMS = [\n",
        "    (\"ICD9-DIAG\", \"DIAGNOSES_ICD.csv.gz\", \"ICD9_CODE\", \"icd9_diag\"),\n",
        "    (\"ICD9-PROC\", \"PROCEDURES_ICD.csv.gz\", \"ICD9_CODE\", \"icd9_proc\"),\n",
        "]\n",
        "MIN_TARGET_COUNT = 10  # Minimum number of times a code must appear to be included\n",
        "preprocessor = TextPreprocessor(\n",
        "    lower=True,\n",
        "    remove_special_characters_mullenbach=True,\n",
        "    remove_special_characters=False,\n",
        "    remove_digits=True,\n",
        "    remove_accents=False,\n",
        "    remove_brackets=False,\n",
        "    convert_danish_characters=False,\n",
        ")\n",
        "\n",
        "random.seed(10)\n",
        "\n",
        "\n",
        "# The dataset requires a Licence in physionet. Once it is obtained, download the dataset with the following command in the terminal:\n",
        "# wget -r -N -c -np --user <your_physionet_user_name> --ask-password https://physionet.org/files/mimiciii/1.4/\n",
        "# Change the path of DOWNLOAD_DIRECTORY to the path where you downloaded mimiciii\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "download_dir = Path(DOWNLOAD_DIRECTORY_MIMICIII)\n",
        "output_dir = Path(DATA_DIRECTORY_MIMICIII_CLEAN)\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "def get_duplicated_icd9_proc_codes() -> set:\n",
        "    \"\"\"Get the duplicated ICD9-PROC codes. The codes are duplicated because they are saved as integers,\n",
        "    removing any zeros at the beginning of the codes. These codes will not be included in the dataset.\n",
        "\n",
        "    Returns:\n",
        "        set: The duplicated ICD9-PROC codes\n",
        "    \"\"\"\n",
        "    icd9_proc_codes = pd.read_csv(\n",
        "        download_dir / \"D_ICD_PROCEDURES.csv.gz\",\n",
        "        compression=\"gzip\",\n",
        "        dtype={\"ICD9_CODE\": str},\n",
        "    )\n",
        "    return set(\n",
        "        icd9_proc_codes[icd9_proc_codes[\"ICD9_CODE\"].astype(str).duplicated()][\n",
        "            \"ICD9_CODE\"\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "def prepare_discharge_summaries(mimic_notes: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Format the notes dataframe into the discharge summaries dataframe\n",
        "\n",
        "    Args:\n",
        "        mimic_notes (pd.DataFrame): The notes dataframe\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Formatted discharge summaries dataframe\n",
        "    \"\"\"\n",
        "    mimic_notes = mimic_notes.rename(\n",
        "        columns={\n",
        "            \"HADM_ID\": ID_COLUMN,\n",
        "            \"SUBJECT_ID\": SUBJECT_ID_COLUMN,\n",
        "            \"TEXT\": TEXT_COLUMN,\n",
        "        }\n",
        "    )\n",
        "    logging.info(f\"{mimic_notes[ID_COLUMN].nunique()} number of admissions\")\n",
        "    discharge_summaries = merge_reports_addendum(mimic_notes)\n",
        "    discharge_summaries = discharge_summaries.sort_values(\n",
        "        [SUBJECT_ID_COLUMN, ID_COLUMN]\n",
        "    )\n",
        "\n",
        "    discharge_summaries = discharge_summaries.reset_index(drop=True)\n",
        "    logging.info(\n",
        "        f\"{discharge_summaries[SUBJECT_ID_COLUMN].nunique()} subjects, {discharge_summaries[ID_COLUMN].nunique()} admissions\"\n",
        "    )\n",
        "    return discharge_summaries\n",
        "\n",
        "\n",
        "def filter_codes(df: pd.DataFrame, columns: list[str], min_count: int) -> pd.DataFrame:\n",
        "    \"\"\"Filter the codes dataframe to only include codes that appear at least min_count times\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The codes dataframe\n",
        "        col (str): The column name of the codes\n",
        "        min_count (int): The minimum number of times a code must appear\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The filtered codes dataframe\n",
        "    \"\"\"\n",
        "    for col in columns:\n",
        "        code_counts = Counter([code for codes in df[col] for code in codes])\n",
        "        codes_to_keep = set(\n",
        "            code for code, count in code_counts.items() if count >= min_count\n",
        "        )\n",
        "        df[col] = df[col].apply(lambda x: [code for code in x if code in codes_to_keep])\n",
        "    return df\n",
        "\n",
        "\n",
        "def download_and_preprocess_code_systems(code_systems: list[tuple]) -> pd.DataFrame:\n",
        "    \"\"\"Download and preprocess the code systems dataframe\n",
        "\n",
        "    Args:\n",
        "        code_systems (List[tuple]): The code systems to download and preprocess\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The preprocessed code systems dataframe\"\"\"\n",
        "    code_dfs = []\n",
        "    for name, fname, col_in, col_out in code_systems:\n",
        "        logging.info(f\"Loading {name} codes...\")\n",
        "        df = pd.read_csv(\n",
        "            download_dir / fname, compression=\"gzip\", dtype={\"ICD9_CODE\": str}\n",
        "        )\n",
        "        df = format_code_dataframe(df, col_in, col_out)\n",
        "        df = remove_duplicated_codes(df, [col_out])\n",
        "        code_dfs.append(df)\n",
        "\n",
        "    merged_codes = merge_code_dataframes(code_dfs)\n",
        "    merged_codes = replace_nans_with_empty_lists(merged_codes)\n",
        "    merged_codes[\"icd9_diag\"] = merged_codes[\"icd9_diag\"].apply(\n",
        "        lambda codes: list(map(partial(reformat_icd9, is_diag=True), codes))\n",
        "    )\n",
        "    merged_codes[\"icd9_proc\"] = merged_codes[\"icd9_proc\"].apply(\n",
        "        lambda codes: list(map(partial(reformat_icd9, is_diag=False), codes))\n",
        "    )\n",
        "    merged_codes[TARGET_COLUMN] = merged_codes[\"icd9_proc\"] + merged_codes[\"icd9_diag\"]\n",
        "    return merged_codes\n",
        "\n",
        "\n",
        "get_duplicated_icd9_proc_codes()\n",
        "# MIMIC-III full\n",
        "mimic_notes = get_mimiciii_notes(download_dir)\n",
        "discharge_summaries = prepare_discharge_summaries(mimic_notes)\n",
        "merged_codes = download_and_preprocess_code_systems(CODE_SYSTEMS)\n",
        "\n",
        "full_dataset = discharge_summaries.merge(\n",
        "    merged_codes, on=[SUBJECT_ID_COLUMN, ID_COLUMN], how=\"inner\"\n",
        ")\n",
        "full_dataset = replace_nans_with_empty_lists(full_dataset)\n",
        "# Remove codes that appear less than 10 times\n",
        "full_dataset = filter_codes(\n",
        "    full_dataset, [TARGET_COLUMN, \"icd9_proc\", \"icd9_diag\"], min_count=MIN_TARGET_COUNT\n",
        ")\n",
        "# Remove admissions with no codes\n",
        "full_dataset = full_dataset[full_dataset[TARGET_COLUMN].apply(len) > 0]\n",
        "\n",
        "full_dataset = preprocess_documents(df=full_dataset, preprocessor=preprocessor)\n",
        "\n",
        "logging.info(f\"{full_dataset[ID_COLUMN].nunique()} number of admissions\")\n",
        "full_dataset = full_dataset.reset_index(drop=True)\n",
        "full_dataset.to_feather(output_dir / \"mimiciii_clean.feather\")\n",
        "logging.info(f\"Saved full dataset to {output_dir / 'mimiciii_clean.feather'}\")"
      ],
      "metadata": {
        "id": "efE7JL2j6r0j",
        "outputId": "41cb3d49-166c-4bca-913f-89ea612cf371",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'prepare_data'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-884879393.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m from prepare_data.utils import (\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mTextPreprocessor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mformat_code_dataframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'prepare_data'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "4OIF_SfPBPx_"
      }
    }
  ]
}